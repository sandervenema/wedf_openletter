{% extends "petitions/layout.html" %}
{% load i18n %}

{% block title %}{% trans "Sign the open letter!" %}{% endblock %}

{% block content %}
    {% get_available_languages as LANGUAGES %}
    {% get_language_info_list for LANGUAGES as languages %}

    <form id="language_form" action="{% url 'set_language' %}" method="post">
        {% csrf_token %}
        <input name="next" type="hidden" value="{{ request.path }}">
        <select name="language">
        {% for language in languages %}
            <option value="{{ language.code }}">{{ language.name_local }}</option>
        {% endfor %}
        </select>
    </form>

    <div class="row language-switcher">
        <ul class="languages">
        {% for language in languages %}
            <li><a href="#" data-language-code="{{ language.code }}"><img src="/static/images/flags/{{ language.code }}.png"> {{ language.name_local }}</a></li>
        {% endfor %}
        </ul>
    </div>

    <div class="row header">
        <div class="small-12 medium-8 column logo">
            <p><a href="https://worldethicaldata.org" target="_blank">
                <img src="/static/images/logo.png" alt="WEDF" style="width: 381px; height: auto;">
            </a><p>
        </div>
    </div>

    <div class="row">
        <div class="small-12 medium-8 column">
            <h2>Overview</h2>

            <p>This is an Open Suggestion designed to open up the process of how AI is built with the steps that go into building responsible AI. It is written from the frontlines by the actual builders, users, and stakeholders who have seen the value and damage Artificial Intelligence (AI) can deliver. The goal is to set a healthy tone for the industry while making the process understandable by the public to illuminate how we can build more ethical AI.</p>
            
            <p>Today we're seeing many doomerism narratives that play into hysteria and obfuscate the structures of AI instead of focusing on clear, accountable communication that enables honest discussion and assists decision making. The creation of AI can be a technically involved process but the standard approaches to teach machines and the outputs created can be monitored and validated in this simple framework that enables everyone to have a seat at the table regardless of a person's data science background.</p>
            
            <p>Now more than ever, it's necessary to open up the process so that we can hold each other accountable without severely restricting innovation. The development of AI has the potential to bring about significant benefits, including increased efficiency, improved decision making, health discoveries and the ability to tackle problems that were previously impossible to solve. However, AI also has the ability to force human moderators to witness horrible content, disadvantaged the disenfranchised, amplify data privacy and safety concerns, steal intellectual property not created for the process, and perpetuate biases that cause harm.  Without careful consideration and planning, we risk creating AI systems that worsen the very problems we seek to solve.</p>
            
            <h2>Open Suggestion</h2>
            
            <p>These are the first questions and considerations we believe every AI team and individual builder should take everyday to ensure we are releasing more ethical AI models. They are intended to be communicated in simple language without technical jargon to ensure the process  can be understood by every audience and will be translated over time into every local human language where we can find volunteers to support. We know we are unlikely to have got this completely right, esp. when dealing with a technology that requires continuous tracking, but with your help each iteration will bring meaningful improvements. This is v1, the first version of questions that developers can use when assessing our work. Eventually, it will evolve into v294029 as we refine the list to capture the necessary steps that should be taken to clarify and validate our intentions.</p>
            
            <p>As policy makers frantically race to catch-up with regulation and the EU AI Act signals it may add GPAI (General Purpose AI) to the high risk category, we are here to set an actionable standard every team can use immediately that also clarifies how AI is built so we can reduce silos and discuss as a group. Instead of an Open Letter, this is an Open Suggestion from a group of experienced technologists, data scientists, researchers, and people concerned with the impact of AI. </p>
            
            <p>This Open Suggestion will live as a free online forum, and we invite new suggestions and approaches under the important banner of registering intent. The steps are isolated based on the core elements of building AI (Training, Building, Testing) and the actors who engage in the process to help clarify the importance of silo reduction: Me, We, It. </p>
            
            <p>Me - The questions each individual who is working on the AI should ask themselves before they start and as they work through the process. </p>
            
            <p>We - The questions the group should ask themselves and define the diversity required to reduce as much human bias as possible. </p>
            
            <p>It - The questions we should ask individuals and the group as they relate to the model being created and the impact it can have on our world.</p>
            
            <h2>Framework</h2>

            <div id="accordion">
                <h3>Step 1. Training  - Data Selection and Ingestion</h3>

                <div>
                    <p>Me - The necessary questions to ask myself before starting this part of the process. Answers should be saved so you can look back at how your thought process evolves.</p>

                    <ul>
                        <li>What are the reasons for me selecting this training data, and how does that selection align with my intention for this model? </li>
                        <li>Do I have experience in using similar data for AI or Machine Learning models in the past or is this the first time I am using this data source?</li>
                        <li>If I've used this data previously, were there any issues that resulted from the models that were trained on this data historically? </li>
                        <li>If this is the first time I've used this data, what are my expectations for the impact this data will have on the models outputs?</li>
                        <li>Have you used a Model Card to communicate risks and how will this document be updated with the collaboration team? </li>
                        <li>What do I hope the data will do to this model, what is my intention of the outcome and how do I expect the training data will impact that performance?</li>
                        <li>Can I summarise what is in this training data in a way that captures the essence of the data in a way a non-data scientist can understand?</li>
                        <li>Do I feel rushed or pressured to input data from questionable sources?</li>
                        <li>Can I cite my source of the training data?</li>
                        <li>What biases may be acting on my selection of this data? </li>
                        <li>Am I considering biases I have that I don't understand and am I sharing my logic with a larger group who can help me identify my bias being deployed when selecting data?</li>
                        <li>How do I think this training data will benefit this model?</li>
                    </ul>

                    <p>We - The questions we should ask ourselves as a working group before starting this part of the process. Answers should be saved so we can look back at how our thought process evolves.                    </p>

                    <ul>
                        <li>Who collaborated in the process of building the training data strategy and selection?</li>
                        <li>Is the team of people who are working on selecting the training data from a diverse set of backgrounds and experiences to help reduce the bias in the data selection?</li>
                        <li>What are the likely biases inherent in this team that selected the training data?</li>
                        <li>Does the whole team understand where the train data came from, and can they explain it back in their own words?</li>
                        <li>Is a Model Card being used to communicate risks and did each contributor create their own document that can be shared with the larger team?</li>
                        <li>Have we considered the EU AI Act or any other regulation being proposed or that is already in place?</li>
                        <li>Is there any protected or copyrighted material in the training data such as Personally Identifiable Information (PII), Payment Card Industry (PCI) data, and Protected Health Information (PHI), and have we considered GDPR, CCPA or any other systems for managing data sources?</li>
                        <li>What percentage of my training data am I saving for testing and how am I selecting it?</li>
                    </ul>

                    <p>It - The questions we should ask ourselves of the algorithms or models before starting this part of the process. Answers should be saved so we can look back at how our thought process evolves. </p>

                    <ul>
                        <li>Do I have means to compensate should part of this data set be discovered to be illegal, unreliable, or unacceptable at some point in the future? </li>
                        <li>What data is needed to train thoughtfully and with intention? </li>
                        <li>Is the full data set of known origin, explainable, and beneficial for the model? </li>
                        <li>What are the likely biases inherent in the data? </li>
                        <li>Is there any personal identifiable information, protected data, or copyrighted material?</li>
                        <li>Am I prepared to handle liability for the model if part of the data set causes any legal issues in the future?</li>
                    </ul>
                </div>

                <h3>Step 2. Building - Creation or Selection of Algorithms and Models</h3>

                <div>
                    <p>Me - The questions I should ask myself before starting this part of the process. Answers should be saved so you can look back at how your thought process evolves. </p>

                    <ul>
                        <li>What do I intend for this model to do, and why am I training it?</li>
                        <li>If I am running reinforcement learning, how will this model optimise in the live environment and is it possible my selection of outcomes to test is biassed?</li>
                        <li>If I am deploying transfer learning, what are the possible biases that the transferring process will uncover?</li>
                        <li>If I am running ensemble models or systems that train each other is there a chance that new bias or bad data collection practices will enter the system?</li>
                        <li>How do I think this model will perform and what are some examples of desired outputs I am hoping to see?</li>
                        <li>What are my human biases that I possess that impacted my goals and reasoning?</li>
                        <li>If I didn't write the model from scratch, where did it come from and how was it initially trained?</li>
                    </ul>
                    
                    <p>We - The questions we should ask ourselves as a working group before starting this part of the process. Answers should be saved so we can look back at how our thought process evolves. </p>
                    
                    <ul>
                        <li>Whom did I collaborate with in the process of training this model and building the strategy?</li>
                        <li>What human biases are in this group, and have we considered if the working group is diverse enough to capture differing points of view?</li>
                        <li>Who were the collaborators building the model and strategy? </li>
                        <li>Who are the stakeholders and are all the stakeholders engaging in this step of the process?</li>
                        <li>Have we considered the EU AI Act or any other regulation being proposed or that is already in place?</li>
                        <li>Were these models trained on any protected or copyrighted material such as Personally Identifiable Information (PII), Payment Card Industry (PCI) data, and Protected Health Information (PHI), and have we considered GDPR, CCPA or any other systems for managing data sources?</li>
                    </ul>

                    <p>It - The questions we should ask ourselves of the algorithms or models before starting this part of the process. Answers should be saved so we can look back at how our thought process evolves. </p>
                    
                    <ul>
                        <li>Where did the model come from or was it developed from scratch? </li>
                        <li>What is the intended use of the model once it is trained? </li>
                        <li>What are potential unintended uses/consequences, including subsequent level outcomes?</li>
                        <li>What are the likely biases that could be amplified through the model?</li>
                        <li>Do we have examples of these models with similar data being deployed previously and what were the intended and unintended outcomes?</li>
                        <li>What are the possible dangers of the model and do we have a plan for the worst case scenarios? </li>
                        <li>What type of measure have I put into place as precautionary measures? </li>
                        <li>Who controls the model? </li>
                        <li>How will continued compliance with laws and regulations be monitored and implemented?</li>
                        <li>How will biases be discovered and resolved? </li>
                        <li>How can the model be shut down and under what circumstances must that happen? </li>
                        <li>Are there any self interested stakeholders or realities of funding that may stop that from happening?</li>
                        <li>What are the contractual requirements that have been entered into that will dictate the management and usage of this model?</li>
                    </ul>
                </div>

                <h3>Step 3. Testing - Managing Test Data and Tagging</h3>
            
                <div>
                    <p>Me - The questions I should ask myself before starting this part of the process. Answers should be saved so you can look back at how your thought process evolves.</p>
        
                    <ul>
                        <li>Is the data I'm using for testing sufficient for analysing how the model performs?</li>
                        <li>Is user testing in the live environment being considered and how will the model be adapted if it turns out that the user behaviour outcome is unwanted. </li>
                        <li>What do I think is the best approach for evaluating the outcome of this model before I speak with the larger group about our approach?</li>
                        <li>Am I confident that the team and I are aware of the impact our work can have and have we thought about all the potentially bad outcomes that could result from our work that we should be testing for before we go live?</li>
                        <li>Do I believe we have been thorough in our testing strategy and deployment to ensure we have unearthed any issues that could arise from the models outputs?</li>
                    </ul>
        
                    <p>We - The questions we should ask ourselves as a working group before starting this part of the process. Answers should be saved so we can look back at how our thought process evolves.</p>
                    <ul>
                        <li>How do we evaluate the performance and outcomes of this model?</li>
                        <li>Are we aware of where the testing data come from and is the testing data appropriate to be representative in relation to the vastness of the training data?</li>
                        <li>If the data is tagged by people, who are the people, are they being humanely treated?</li>
                        <li>What instruction did taggers receive before they tag the data that might impact their opinion?</li>
                        <li>What questions are the taggers or working group asking of the data?</li>
                        <li>Does the tagging strategy align with the training data and model creation strategy?</li>
                        <li>Is there an appropriate amount of diversity on my data tagging team?</li>
                        <li>What human biases might impact the tagging or testing process?</li>
                        <li>Is user testing in the live environment data being collected and how will it feed back into an iterative process?</li>
                        <li>Are the user group a diverse audience that is representative of the future total user group? </li>
                        <li>Is it possible that bias is a programmatic result of the model?</li>
                        <li>Whom did I collaborate with in the process of building the testing or tagging data strategy?</li>
                        <li>Are product teams involved in the process of retrieving user data and sharing human logic with the larger group?</li>
                    </ul>
                    
                    <p>It - The questions we should ask ourselves of the algorithms or models before starting this part of the process. Answers should be saved so we can look back at how our thought process evolves.</p>
                    
                    <ul>
                        <li>Did the results of the test or tagged data match our intent for the model outputs?</li>
                        <li>If we are testing our models to determine accuracy, how much do the results we are seeing match up with the core intent of what the model will do? </li>
                        <li>Will I now tune the model based on the outputs of the testing or tagged data? If so, start the process from Step 1.</li>
                        <li>If deploying reinforcement learning, how can model outcomes amplify bias and influence new data from users?</li>
                        <li>Have we considered the output of the model against the EU AI Act or any other regulation being proposed or that is already in place?</li>
                        <li>Have we tested for protected or copyrighted material in the output of the model such as Personally Identifiable Information (PII), Payment Card Industry (PCI) data, and Protected Health Information (PHI), and have we considered GDPR, CCPA or any other systems for managing data sources?</li>
                    </ul>
                </div>
            </div>

            <hr>

            <div>
                <h4>{% trans "Signatures:" %}</h4>

                {% for sig in signatures %}
                    {% if forloop.counter0|divisibleby:2 %}
                        <div class="row">
                    {% endif %}

                    <div class="small-12 medium-6 column">
                        <p class="signame">{{ sig.name }}<br><span class="affil">{{ sig.affiliation }}</span></p>
                    </div>

                    {% if forloop.counter|divisibleby:2 %}
                        </div>
                    {% endif %}
                    {% if forloop.last and forloop.counter0|divisibleby:2 %}
                        </div>
                    {% endif %}
                {% endfor %}
            </div>
        </div>

        <div class="small-12 medium-4 column panel">
            <h2>{% trans "Join the fight for more ethical AI!" %}</h2>

            <p>{% trans "This is version 1 of the perennial task of collecting best practices for AI. If you think this is a good starting place, show your support by adding your signature:" %}</p>

            <p>{% trans "Number of new signatures since launch:" %} <strong>{{ petition.signatures }}</strong></p>

            <form action="{% url 'sign' %}" method="post">
                {% csrf_token %}
                {{ form }}

                <input type="submit" value="{% trans "Add!" %}" class="button radius btn-success">
            </form>

            <p><small>{% blocktrans %}
            Your e-mail address will not be stored by the system, and
            will only be used to send you an e-mail once, with a
            confirmation link to confirm your signature.
            {% endblocktrans %}</small></p>
        </div>
    </div>
{% endblock %}
